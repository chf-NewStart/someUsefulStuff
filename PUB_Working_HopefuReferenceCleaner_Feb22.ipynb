{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM-only Reference Cleaner (DOCX -> DOCX) – User Manual\n",
        "\n",
        "## What this tool does\n",
        "\n",
        "This notebook takes a messy References section from a Word document and converts it into a clean, consistent reference list using an LLM.\n",
        "\n",
        "It can:\n",
        "- split glued references into individual entries  \n",
        "- reformat each reference into a target style  \n",
        "- optionally sort references alphabetically by first author  \n",
        "- apply numbering styles (1., [1], A., or none)  \n",
        "- export a clean References section as a new .docx file  \n",
        "\n",
        "---\n",
        "\n",
        "## What you need before running\n",
        "\n",
        "- A Google Colab notebook  \n",
        "- A DeepSeek or OpenAI API key set as an environment variable  \n",
        "- Your messy references file in .docx format  \n",
        "- (Optional) A clean references .docx file to infer style from  \n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Set your API key\n",
        "\n",
        "In Colab, run this in a cell:\n",
        "\n",
        "export DEEPSEEK_API_KEY=\"your_api_key_here\"\n",
        "\n",
        "or for OpenAI:\n",
        "\n",
        "export OPENAI_API_KEY=\"your_api_key_here\"\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2: Run the main script cell\n",
        "\n",
        "Just run the full Python cell that contains the pipeline code.\n",
        "\n",
        "When prompted in the terminal:\n",
        "\n",
        "do you want your target numbering sorting be alphabetical(a) or keep(k) ?\n",
        "- a = sort references alphabetically by first author\n",
        "- k = keep original order\n",
        "\n",
        "do you want your target numbering style be dot(d) | brackets(b) | alpha(a) | none(n) ?\n",
        "- d = 1. Reference\n",
        "- b = [1] Reference\n",
        "- a = A. Reference\n",
        "- n = Reference (no numbering)\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3: Upload your files\n",
        "\n",
        "You will be prompted to upload:\n",
        "\n",
        "1) SOURCE .docx  \n",
        "   This is your messy reference file.\n",
        "\n",
        "2) TARGET .docx (optional)  \n",
        "   This is a clean reference list.  \n",
        "   If you upload this, the model will infer the formatting style.  \n",
        "   If you cancel, a default formatting style is used.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 4: What happens automatically\n",
        "\n",
        "The pipeline runs these steps:\n",
        "\n",
        "1) Extracts only the References section from your SOURCE docx  \n",
        "2) Uses LLM to split glued references into a list  \n",
        "3) (Optional) Sorts references alphabetically by first author  \n",
        "4) Uses LLM to reformat each reference into target style  \n",
        "5) Adds numbering in Python  \n",
        "6) Writes a new References section into a clean .docx file  \n",
        "\n",
        "---\n",
        "\n",
        "## Step 5: Download result\n",
        "\n",
        "After the run finishes, Colab will automatically download:\n",
        "\n",
        "references_llm_fixed.docx\n",
        "\n",
        "This file contains:\n",
        "- cleanly formatted references  \n",
        "- consistent style  \n",
        "- correct numbering  \n",
        "- hanging indent  \n",
        "\n",
        "---\n",
        "\n",
        "## Common issues\n",
        "\n",
        "If it crashes with “Failed to get valid JSON”:\n",
        "- Reduce BATCH_SIZE (e.g. from 6 to 4)\n",
        "- Try running again (LLMs are stochastic)\n",
        "\n",
        "If References heading is not found:\n",
        "- Make sure your Word doc has a line exactly like:\n",
        "  References\n",
        "\n",
        "If sorting looks wrong:\n",
        "- Alphabetical sorting is based on first author surname extracted by LLM  \n",
        "- Rare edge cases may mis-order unusual names  \n",
        "\n",
        "---\n",
        "\n",
        "## Tips\n",
        "\n",
        "- If your target journal does NOT want DOIs, remove them from your target reference file before uploading it  \n",
        "- If your references contain non-English names, alphabetical sorting may be slightly imperfect  \n",
        "- You can rerun with different numbering styles without re-uploading your files  \n",
        "\n",
        "---\n",
        "\n",
        "## Output\n",
        "\n",
        "Final file:\n",
        "references_llm_fixed.docx\n",
        "\n",
        "You can copy this directly into your paper."
      ],
      "metadata": {
        "id": "5sBqiKcS2pmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install openai python-docs"
      ],
      "metadata": {
        "id": "iwe5nGx1hQdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "from openai import OpenAI\n",
        "\n",
        "ds_key = \"YOUR-OWN-KEY\"\n",
        "\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = ds_key  # or use Colab secrets\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
        "    base_url=\"https://api.deepseek.com/v1\"\n",
        ")"
      ],
      "metadata": {
        "id": "sYAPpS-pvFcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# LLM-only Reference Cleaner (DOCX -> DOCX) for Google Colab\n",
        "# Robust + stepwise + never-crash batch formatting + optional alphabetical sort\n",
        "#\n",
        "# What it does:\n",
        "# 1) Upload SOURCE .docx (messy references like your scr2)\n",
        "# 2) (Optional) Upload TARGET .docx (clean reference style like scr3)\n",
        "# 3) Extract ONLY the References section text\n",
        "# 4) STEP 1: LLM splits glued references into individual items\n",
        "# 5) STEP 1.5: Optional sort (alphabetical by first author surname using LLM-extracted key)\n",
        "# 6) STEP 2: LLM formats refs in batches using index mapping (no batch mismatch)\n",
        "# 7) Python adds numbering prefixes (dot / brackets / alpha / none)\n",
        "# 8) Writes a clean output DOCX with hanging indent\n",
        "#\n",
        "# =========================\n",
        "\n",
        "!pip -q install python-docx openai\n",
        "\n",
        "import os, re, json, time\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "from google.colab import files\n",
        "from openai import OpenAI\n",
        "\n",
        "# -------------------------\n",
        "# USER CONFIG\n",
        "# -------------------------\n",
        "PROVIDER = \"deepseek\"      # \"deepseek\" or \"openai\"\n",
        "MODEL = \"deepseek-chat\"    # deepseek: \"deepseek-chat\"; openai: \"gpt-4o-mini\" / \"gpt-4.1-mini\"\n",
        "DEEPSEEK_BASE_URL = \"https://api.deepseek.com\"\n",
        "\n",
        "DEBUG = True\n",
        "DEBUG_SHOW_JSON = False   # True -> prints sample JSON returned by model (can be noisy)\n",
        "BATCH_SIZE = 6            # 4-6 stable; 8-10 faster but riskier\n",
        "SPLIT_CHUNK_CHARS = 6000  # chunk size for splitting step\n",
        "\n",
        "# Sorting\n",
        "SORT_MODE = input(\"do you want your target numbering sorting be alphabetical(a) or keep(k) ?\")    # \"keep\" or \"alphabetical\"\n",
        "# Numbering prefix added in Python AFTER LLM formatting (LLM is told: no numbering)\n",
        "NUMBERING_STYLE = input(\"do you want your target numbering style be dot(d) | brackets(b) | alpha(a) | none(n) ?\")\n",
        "\n",
        "# Output\n",
        "OUT_PATH = \"references_llm_fixed.docx\"\n",
        "\n",
        "# -------------------------\n",
        "# Client\n",
        "# -------------------------\n",
        "def make_client() -> OpenAI:\n",
        "    if PROVIDER == \"deepseek\":\n",
        "        key = (os.environ.get(\"DEEPSEEK_API_KEY\") or \"\").strip()\n",
        "        if not key:\n",
        "            raise RuntimeError(\"Missing DEEPSEEK_API_KEY. Add it to Colab Secrets or os.environ.\")\n",
        "        return OpenAI(api_key=key, base_url=DEEPSEEK_BASE_URL)\n",
        "\n",
        "    if PROVIDER == \"openai\":\n",
        "        key = (os.environ.get(\"OPENAI_API_KEY\") or \"\").strip()\n",
        "        if not key:\n",
        "            raise RuntimeError(\"Missing OPENAI_API_KEY. Add it to Colab Secrets or os.environ.\")\n",
        "        return OpenAI(api_key=key)\n",
        "\n",
        "    raise ValueError(\"PROVIDER must be 'deepseek' or 'openai'\")\n",
        "\n",
        "client = make_client()\n",
        "\n",
        "# -------------------------\n",
        "# Robust JSON extraction\n",
        "# -------------------------\n",
        "def extract_first_json_balanced(text: str) -> str:\n",
        "    \"\"\"Extract the first complete JSON object/array using brace/bracket balancing.\"\"\"\n",
        "    if not text:\n",
        "        raise ValueError(\"Empty model output\")\n",
        "\n",
        "    s = text.strip()\n",
        "    start = None\n",
        "    for i, ch in enumerate(s):\n",
        "        if ch in \"{[\":\n",
        "            start = i\n",
        "            break\n",
        "    if start is None:\n",
        "        raise ValueError(\"No JSON start found\")\n",
        "\n",
        "    opener = s[start]\n",
        "    closer = \"}\" if opener == \"{\" else \"]\"\n",
        "\n",
        "    depth = 0\n",
        "    in_str = False\n",
        "    esc = False\n",
        "\n",
        "    for j in range(start, len(s)):\n",
        "        ch = s[j]\n",
        "        if in_str:\n",
        "            if esc:\n",
        "                esc = False\n",
        "            elif ch == \"\\\\\":\n",
        "                esc = True\n",
        "            elif ch == '\"':\n",
        "                in_str = False\n",
        "            continue\n",
        "        else:\n",
        "            if ch == '\"':\n",
        "                in_str = True\n",
        "                continue\n",
        "            if ch == opener:\n",
        "                depth += 1\n",
        "            elif ch == closer:\n",
        "                depth -= 1\n",
        "                if depth == 0:\n",
        "                    return s[start:j+1]\n",
        "\n",
        "    raise ValueError(\"Unbalanced JSON (never closed)\")\n",
        "\n",
        "def parse_json_robust(text: str) -> Any:\n",
        "    t = (text or \"\").strip()\n",
        "    if not t:\n",
        "        raise ValueError(\"Empty response\")\n",
        "    try:\n",
        "        return json.loads(t)\n",
        "    except json.JSONDecodeError:\n",
        "        blob = extract_first_json_balanced(t)\n",
        "        return json.loads(blob)\n",
        "\n",
        "def llm_json(prompt: str, temperature: float = 0.0, max_retries: int = 3, max_tokens: int = 6000) -> Any:\n",
        "    sys = \"Return ONLY valid JSON. No markdown. No commentary. No extra text.\"\n",
        "    last_txt = None\n",
        "    cur_prompt = prompt\n",
        "\n",
        "    for _ in range(max_retries):\n",
        "        resp = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": sys},\n",
        "                {\"role\": \"user\", \"content\": cur_prompt},\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            stop=[\"```\"],\n",
        "            max_tokens=max_tokens,\n",
        "        )\n",
        "        txt = resp.choices[0].message.content or \"\"\n",
        "        last_txt = txt\n",
        "\n",
        "        try:\n",
        "            return parse_json_robust(txt)\n",
        "        except Exception:\n",
        "            cur_prompt = f\"\"\"\n",
        "You output invalid JSON or extra text. Output ONLY valid JSON for the schema requested.\n",
        "No markdown. No explanations.\n",
        "\n",
        "BAD_OUTPUT:\n",
        "{txt}\n",
        "\"\"\"\n",
        "            time.sleep(0.25)\n",
        "\n",
        "    raise ValueError(f\"Failed to get valid JSON after retries. Last output:\\n{(last_txt or '')[:900]}\")\n",
        "\n",
        "# -------------------------\n",
        "# DOCX: find References and extract only that section\n",
        "# -------------------------\n",
        "def norm(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip()).lower()\n",
        "\n",
        "def find_references_start(doc: Document) -> int:\n",
        "    # 1) exact-ish heading\n",
        "    for i, p in enumerate(doc.paragraphs):\n",
        "        t = norm(p.text)\n",
        "        if t in {\"references\", \"reference\", \"bibliography\", \"literature cited\"}:\n",
        "            return i\n",
        "        if re.fullmatch(r\"references\\s*:?\", t):\n",
        "            return i\n",
        "    # 2) fallback: short line containing \"references\"\n",
        "    for i, p in enumerate(doc.paragraphs):\n",
        "        t = norm(p.text)\n",
        "        if \"references\" in t and len(t) <= 35:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "def extract_text_after_heading(doc: Document, heading_idx: int) -> str:\n",
        "    lines = []\n",
        "    for p in doc.paragraphs[heading_idx+1:]:\n",
        "        txt = (p.text or \"\").strip()\n",
        "        if not txt:\n",
        "            continue\n",
        "        # stop at an obvious next heading (very rough)\n",
        "        if len(txt) <= 25 and txt.isupper() and \"REFER\" not in txt.upper():\n",
        "            break\n",
        "        lines.append(txt)\n",
        "    return \"\\n\".join(lines).strip()\n",
        "\n",
        "def extract_references_section_text(doc: Document) -> str:\n",
        "    idx = find_references_start(doc)\n",
        "    if idx < 0:\n",
        "        raise ValueError(\"Could not find a 'References' heading. Make sure there is a line that says: References\")\n",
        "    txt = extract_text_after_heading(doc, idx)\n",
        "    if not txt.strip():\n",
        "        raise ValueError(\"Found References heading but extracted no text after it.\")\n",
        "    return txt\n",
        "\n",
        "# -------------------------\n",
        "# Step 0: infer style from target references text (robust schema)\n",
        "# -------------------------\n",
        "DEFAULT_STYLE = {\n",
        "    \"authors_rule\": \"Use 'Family, Initials' and separate authors with '; ' (semicolon + space). Keep author order. Do not invent authors.\",\n",
        "    \"year_rule\": \"Use 4-digit year if present; place it after authors or after journal consistent with target.\",\n",
        "    \"journal_vol_issue_pages_rule\": \"Keep journal/volume/issue/pages ordering consistent with target. Do not invent missing fields.\",\n",
        "    \"title_rule\": \"Keep title as-is (do not invent capitalization). End title with a period.\",\n",
        "    \"doi_rule\": \"If DOI exists, include EXACTLY 'https://doi.org/<doi>' at the end unless target excludes DOI.\",\n",
        "    \"one_line_rule\": \"One reference per line. No internal newlines.\"\n",
        "}\n",
        "\n",
        "def infer_style_from_target_text(target_refs_text: str) -> Dict[str, Any]:\n",
        "    prompt = f\"\"\"\n",
        "Infer a reference formatting spec from a clean reference list.\n",
        "\n",
        "Return JSON in either of these shapes:\n",
        "A) {{\"spec\": {{...}}}}\n",
        "B) {{...}}   (the spec directly)\n",
        "\n",
        "Spec keys required:\n",
        "- authors_rule\n",
        "- year_rule\n",
        "- journal_vol_issue_pages_rule\n",
        "- title_rule\n",
        "- doi_rule\n",
        "- one_line_rule\n",
        "\n",
        "TARGET_REFERENCE_LIST_TEXT:\n",
        "{target_refs_text}\n",
        "\"\"\"\n",
        "    data = llm_json(prompt, max_tokens=2500)\n",
        "\n",
        "    if isinstance(data, dict) and \"spec\" in data and isinstance(data[\"spec\"], dict):\n",
        "        spec = data[\"spec\"]\n",
        "    elif isinstance(data, dict):\n",
        "        spec = data\n",
        "    else:\n",
        "        spec = DEFAULT_STYLE.copy()\n",
        "\n",
        "    # force one-line rule regardless (prevents model inserting newlines)\n",
        "    spec[\"one_line_rule\"] = \"One reference per line. No internal newlines.\"\n",
        "    return spec\n",
        "\n",
        "# -------------------------\n",
        "# Step 1: chunk + LLM split glued references into list\n",
        "# -------------------------\n",
        "BRACKET_MARKER = re.compile(r\"\\[\\d+\\]\")\n",
        "\n",
        "def chunk_by_markers(text: str, max_chars: int = 6000) -> List[str]:\n",
        "    t = (text or \"\").strip()\n",
        "    if not t:\n",
        "        return []\n",
        "\n",
        "    positions = [m.start() for m in BRACKET_MARKER.finditer(t)]\n",
        "    if len(positions) <= 1:\n",
        "        # fallback: raw chunking\n",
        "        return [t[i:i+max_chars] for i in range(0, len(t), max_chars)]\n",
        "\n",
        "    chunks = []\n",
        "    start = positions[0]\n",
        "    last = start\n",
        "\n",
        "    for pos in positions[1:]:\n",
        "        if (pos - start) > max_chars:\n",
        "            chunks.append(t[start:last].strip())\n",
        "            start = last\n",
        "        last = pos\n",
        "\n",
        "    chunks.append(t[start:].strip())\n",
        "    return [c for c in chunks if c]\n",
        "\n",
        "def step1_split_refs_llm(raw_text: str) -> List[str]:\n",
        "    chunks = chunk_by_markers(raw_text, max_chars=SPLIT_CHUNK_CHARS)\n",
        "    if not chunks:\n",
        "        raise ValueError(\"No text to split.\")\n",
        "\n",
        "    all_refs = []\n",
        "    for ci, ch in enumerate(chunks, start=1):\n",
        "        prompt = f\"\"\"\n",
        "Return JSON only: {{\"refs\":[...]}}.\n",
        "\n",
        "Task: Split the bibliography text into individual references.\n",
        "\n",
        "Rules:\n",
        "- Each list element must contain EXACTLY ONE reference.\n",
        "- Preserve original content; do NOT rewrite.\n",
        "- Split when a new reference marker begins, like \"[12]\" or \"12.\"\n",
        "- Keep DOI URLs as-is.\n",
        "- Do not add/remove authors, title, year, journal, volume, pages.\n",
        "- Do not merge two references into one list element.\n",
        "- Output refs must be in the same order as they appear.\n",
        "\n",
        "TEXT_CHUNK ({ci}/{len(chunks)}):\n",
        "{ch}\n",
        "\"\"\"\n",
        "        data = llm_json(prompt, max_tokens=6000)\n",
        "\n",
        "        if DEBUG and DEBUG_SHOW_JSON and ci == 1:\n",
        "            print(\"\\n[SPLIT JSON sample]\")\n",
        "            print(json.dumps(data, indent=2, ensure_ascii=False)[:1500])\n",
        "\n",
        "        refs = [r.strip() for r in data.get(\"refs\", []) if r and r.strip()]\n",
        "        all_refs.extend(refs)\n",
        "\n",
        "    if not all_refs:\n",
        "        raise ValueError(\"Split failed: empty refs list.\")\n",
        "    return all_refs\n",
        "\n",
        "# -------------------------\n",
        "# Step 1.5: Optional alphabetical sort (by first author surname)\n",
        "# -------------------------\n",
        "def extract_first_author_surname_llm(ref: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "Return JSON only: {{\"key\":\"...\"}}.\n",
        "\n",
        "Task: Extract the first author's FAMILY NAME (surname) from the reference.\n",
        "Rules:\n",
        "- If you cannot confidently find a surname, return \"zzz\" as key.\n",
        "- Output key should be lowercase ascii if possible; keep letters only.\n",
        "- Do NOT rewrite the reference, just extract a sorting key.\n",
        "\n",
        "REFERENCE:\n",
        "{ref}\n",
        "\"\"\"\n",
        "    data = llm_json(prompt, max_tokens=500)\n",
        "    key = (data.get(\"key\") or \"\").strip().lower()\n",
        "    key = re.sub(r\"[^a-z]\", \"\", key)\n",
        "    return key if key else \"zzz\"\n",
        "\n",
        "def alphabetical_sort_refs(refs: List[str]) -> List[str]:\n",
        "    # cache keys to avoid repeated calls if rerun\n",
        "    keys = []\n",
        "    for r in refs:\n",
        "        k = extract_first_author_surname_llm(r)\n",
        "        keys.append(k)\n",
        "    # stable sort: key then original order\n",
        "    idxs = list(range(len(refs)))\n",
        "    idxs.sort(key=lambda i: (keys[i], i))\n",
        "    return [refs[i] for i in idxs]\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: robust batch formatting (index-based) + retry + per-ref fallback\n",
        "# LLM is told: DO NOT add numbering. Python adds numbering afterwards.\n",
        "# -------------------------\n",
        "def reformat_refs_batch_llm(refs: List[str], style_spec: Dict[str, Any]) -> List[str]:\n",
        "    def _call(batch_refs: List[str]) -> List[Optional[str]]:\n",
        "        prompt = f\"\"\"\n",
        "Return JSON only.\n",
        "\n",
        "Reformat each reference into the target style.\n",
        "\n",
        "STYLE_SPEC:\n",
        "{json.dumps(style_spec, ensure_ascii=False)}\n",
        "\n",
        "Hard rules:\n",
        "- Do NOT invent missing info.\n",
        "- One line per reference (no '\\\\n' inside).\n",
        "- Do NOT merge references.\n",
        "- Do NOT drop any reference.\n",
        "- Do NOT add any numbering prefix (no '1.' no '[1]' no 'A.'). I will add numbering in Python.\n",
        "- Output MUST contain EXACTLY {len(batch_refs)} unique indices.\n",
        "\n",
        "Return JSON EXACT schema:\n",
        "{{\n",
        "  \"items\": [\n",
        "    {{\"index\": 0, \"formatted\": \"...\" }},\n",
        "    {{\"index\": 1, \"formatted\": \"...\" }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Indices must be 0..{len(batch_refs)-1}, no repeats.\n",
        "\n",
        "Input references:\n",
        "{json.dumps(batch_refs, ensure_ascii=False)}\n",
        "\"\"\"\n",
        "        data = llm_json(prompt, max_tokens=6000)\n",
        "\n",
        "        if DEBUG and DEBUG_SHOW_JSON:\n",
        "            print(\"\\n[FORMAT JSON sample]\")\n",
        "            print(json.dumps(data, indent=2, ensure_ascii=False)[:1500])\n",
        "\n",
        "        items = data.get(\"items\", [])\n",
        "        out = [None] * len(batch_refs)\n",
        "\n",
        "        if isinstance(items, list):\n",
        "            for it in items:\n",
        "                if not isinstance(it, dict):\n",
        "                    continue\n",
        "                idx = it.get(\"index\")\n",
        "                fmt = (it.get(\"formatted\") or \"\").strip()\n",
        "                if isinstance(idx, int) and 0 <= idx < len(batch_refs) and fmt:\n",
        "                    # force one-line\n",
        "                    fmt = re.sub(r\"\\s*\\n\\s*\", \" \", fmt).strip()\n",
        "                    out[idx] = fmt\n",
        "        return out\n",
        "\n",
        "    out = _call(refs)\n",
        "    missing = [i for i, v in enumerate(out) if not v]\n",
        "\n",
        "    if missing:\n",
        "        out2 = _call(refs)\n",
        "        for i in missing:\n",
        "            if out2[i]:\n",
        "                out[i] = out2[i]\n",
        "        missing = [i for i, v in enumerate(out) if not v]\n",
        "\n",
        "    if missing:\n",
        "        # per-ref fallback\n",
        "        for i in missing:\n",
        "            single = refs[i]\n",
        "            prompt_single = f\"\"\"\n",
        "Return JSON only: {{\"formatted\":\"...\"}}.\n",
        "\n",
        "Format ONE reference into the target style.\n",
        "\n",
        "STYLE_SPEC:\n",
        "{json.dumps(style_spec, ensure_ascii=False)}\n",
        "\n",
        "Rules:\n",
        "- One line only.\n",
        "- Do not invent missing info.\n",
        "- Do NOT add numbering prefix.\n",
        "\n",
        "REFERENCE:\n",
        "{single}\n",
        "\"\"\"\n",
        "            data = llm_json(prompt_single, max_tokens=2500)\n",
        "            fmt = (data.get(\"formatted\") or \"\").strip()\n",
        "            fmt = re.sub(r\"\\s*\\n\\s*\", \" \", fmt).strip()\n",
        "            out[i] = fmt if fmt else None\n",
        "\n",
        "    final = []\n",
        "    for i, v in enumerate(out):\n",
        "        if not v:\n",
        "            # last resort (never crash)\n",
        "            final.append(refs[i].strip())\n",
        "        else:\n",
        "            final.append(v)\n",
        "    return final\n",
        "\n",
        "# -------------------------\n",
        "# Numbering in Python\n",
        "# -------------------------\n",
        "def alpha_label(n: int) -> str:\n",
        "    # 1->A, 2->B ... 26->Z, 27->AA ...\n",
        "    s = \"\"\n",
        "    while n > 0:\n",
        "        n -= 1\n",
        "        s = chr(ord(\"A\") + (n % 26)) + s\n",
        "        n //= 26\n",
        "    return s\n",
        "\n",
        "def add_prefix(i_global: int, text: str) -> str:\n",
        "    if NUMBERING_STYLE.lower() == \"n\":\n",
        "        return text\n",
        "    if NUMBERING_STYLE.lower() == \"d\":\n",
        "        return f\"{i_global}. {text}\"\n",
        "    if NUMBERING_STYLE.lower() == \"b\":\n",
        "        return f\"[{i_global}] {text}\"\n",
        "    if NUMBERING_STYLE.lower() == \"a\":\n",
        "        return f\"{alpha_label(i_global)}. {text}\"\n",
        "    return f\"{i_global}. {text}\"\n",
        "\n",
        "# -------------------------\n",
        "# Pipeline\n",
        "# -------------------------\n",
        "def reformat_refs_llm_pipeline(raw_text: str, style_spec: Dict[str, Any], batch_size: int = 6) -> List[str]:\n",
        "    refs = step1_split_refs_llm(raw_text)\n",
        "\n",
        "    if DEBUG:\n",
        "        print(\"\\n=== STEP 1 (split) ===\")\n",
        "        print(\"refs:\", len(refs))\n",
        "        print(\"first ref preview:\\n\", refs[0][:320], \"\\n\")\n",
        "\n",
        "    if SORT_MODE.lower() == \"a\":\n",
        "        refs = alphabetical_sort_refs(refs)\n",
        "        if DEBUG:\n",
        "            print(\"=== STEP 1.5 (alphabetical sort) ===\")\n",
        "            print(\"first sorted ref preview:\\n\", refs[0][:320], \"\\n\")\n",
        "\n",
        "    formatted_all = []\n",
        "    cur = 1\n",
        "    for i in range(0, len(refs), batch_size):\n",
        "        chunk = refs[i:i+batch_size]\n",
        "        formatted_chunk = reformat_refs_batch_llm(chunk, style_spec)\n",
        "\n",
        "        # Add numbering prefix in Python globally\n",
        "        for j, s in enumerate(formatted_chunk):\n",
        "            formatted_all.append(add_prefix(cur + j, s))\n",
        "\n",
        "        cur += len(chunk)\n",
        "\n",
        "    if DEBUG:\n",
        "        print(\"=== STEP 2 (formatted) ===\")\n",
        "        print(\"formatted:\", len(formatted_all))\n",
        "        print(\"first formatted preview:\\n\", formatted_all[0][:320], \"\\n\")\n",
        "\n",
        "    return formatted_all\n",
        "\n",
        "# -------------------------\n",
        "# Write DOCX output with hanging indent\n",
        "# -------------------------\n",
        "def set_hanging_indent(paragraph, left_inch=0.35, hanging_inch=0.25):\n",
        "    paragraph.paragraph_format.left_indent = Inches(left_inch)\n",
        "    paragraph.paragraph_format.first_line_indent = Inches(-hanging_inch)\n",
        "\n",
        "def save_formatted_docx(formatted_lines: List[str], out_path: str):\n",
        "    out = Document()\n",
        "    out.add_heading(\"References\", level=1)\n",
        "    for line in formatted_lines:\n",
        "        p = out.add_paragraph(line)\n",
        "        set_hanging_indent(p)\n",
        "    out.save(out_path)\n",
        "\n",
        "# -------------------------\n",
        "# RUN\n",
        "# -------------------------\n",
        "print(\"Upload your SOURCE .docx (messy references).\")\n",
        "up1 = files.upload()\n",
        "if not up1:\n",
        "    raise ValueError(\"No source file uploaded.\")\n",
        "source_path = next(iter(up1.keys()))\n",
        "print(\"Source:\", source_path)\n",
        "\n",
        "print(\"\\nOptional: upload TARGET .docx (clean refs style you want).\")\n",
        "print(\"If you don't have it, click Cancel in the upload dialog.\")\n",
        "try:\n",
        "    up2 = files.upload()\n",
        "    target_path = next(iter(up2.keys())) if up2 else None\n",
        "except Exception:\n",
        "    target_path = None\n",
        "\n",
        "# Extract source refs text\n",
        "src_doc = Document(source_path)\n",
        "raw_text = extract_references_section_text(src_doc)\n",
        "\n",
        "# Build style spec\n",
        "style_spec = DEFAULT_STYLE.copy()\n",
        "if target_path:\n",
        "    tgt_doc = Document(target_path)\n",
        "    target_refs_text = extract_references_section_text(tgt_doc)  # References section only\n",
        "    if DEBUG:\n",
        "        print(\"\\nInferring style from target doc (References section only)...\")\n",
        "    style_spec = infer_style_from_target_text(target_refs_text)\n",
        "\n",
        "# Force one-line rule no matter what\n",
        "style_spec[\"one_line_rule\"] = \"One reference per line. No internal newlines.\"\n",
        "\n",
        "print(\"\\n=== STYLE SPEC IN USE ===\")\n",
        "print(json.dumps(style_spec, indent=2, ensure_ascii=False))\n",
        "\n",
        "# Run pipeline\n",
        "formatted_lines = reformat_refs_llm_pipeline(raw_text, style_spec, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Save + download\n",
        "save_formatted_docx(formatted_lines, OUT_PATH)\n",
        "print(\"Saved:\", OUT_PATH)\n",
        "files.download(OUT_PATH)"
      ],
      "metadata": {
        "id": "i6netUYx4jTo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f7ea4d8-6c6b-480a-d171-8f6060fe036d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do you want your target numbering sorting be alphabetical(a) or keep(k) ?a\n",
            "do you want your target numbering style be dot(d) | brackets(b) | alpha(a) | none(n) ?n\n",
            "Upload your SOURCE .docx (messy references).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4b5cb90f-6268-4094-8549-b29984dd1e26\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4b5cb90f-6268-4094-8549-b29984dd1e26\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ReferencesQ.docx to ReferencesQ (2).docx\n",
            "Source: ReferencesQ (2).docx\n",
            "\n",
            "Optional: upload TARGET .docx (clean refs style you want).\n",
            "If you don't have it, click Cancel in the upload dialog.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c8a87d40-2f69-4fbc-bc02-f23626df1cb9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c8a87d40-2f69-4fbc-bc02-f23626df1cb9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving oriRef.docx to oriRef (2).docx\n",
            "\n",
            "Inferring style from target doc (References section only)...\n",
            "\n",
            "=== STYLE SPEC IN USE ===\n",
            "{\n",
            "  \"authors_rule\": {\n",
            "    \"format\": \"last_name, first_initial., last_name, first_initial., ...\",\n",
            "    \"separator\": \", \",\n",
            "    \"final_separator\": \", \",\n",
            "    \"max_authors\": null,\n",
            "    \"et_al_usage\": \"not used in provided examples\",\n",
            "    \"initials\": \"with period, no space\",\n",
            "    \"ordering\": \"as listed\"\n",
            "  },\n",
            "  \"year_rule\": {\n",
            "    \"format\": \"year\",\n",
            "    \"parentheses\": \"none\",\n",
            "    \"position\": \"after authors, followed by period and space\",\n",
            "    \"separator\": \". \"\n",
            "  },\n",
            "  \"journal_vol_issue_pages_rule\": {\n",
            "    \"format\": \"Journal. Vol.(Issue), Pages.\",\n",
            "    \"journal_abbreviation\": \"standard abbreviated, periods after abbreviated words\",\n",
            "    \"volume_style\": \"Vol.\",\n",
            "    \"issue_style\": \"(Issue),\",\n",
            "    \"pages_style\": \"Pages.\",\n",
            "    \"page_range_separator\": \"–\",\n",
            "    \"no_issue_format\": \"Vol., Pages.\"\n",
            "  },\n",
            "  \"title_rule\": {\n",
            "    \"format\": \"sentence case\",\n",
            "    \"italic\": false,\n",
            "    \"quotes\": false,\n",
            "    \"trailing_period\": true,\n",
            "    \"position\": \"after year\"\n",
            "  },\n",
            "  \"doi_rule\": {\n",
            "    \"format\": \"https://doi.org/...\",\n",
            "    \"prefix\": \"https://doi.org/\",\n",
            "    \"position\": \"after journal/volume/pages, final element\",\n",
            "    \"leading_space\": true\n",
            "  },\n",
            "  \"one_line_rule\": \"One reference per line. No internal newlines.\"\n",
            "}\n",
            "\n",
            "=== STEP 1 (split) ===\n",
            "refs: 24\n",
            "first ref preview:\n",
            " [1] M. Shokouhifar, M. Sohrabi, M. Rabbani, S.M.H. Molana, F. Werner, Sustainable Phosphorus Fertilizer Supply Chain Management to Improve Crop Yield and P Use Efficiency Using an Ensemble Heuristic–Metaheuristic Optimization Algorithm, Agronomy 13(2) (2023) 565,https://doi.org/10.3390/agronomy13020565. \n",
            "\n",
            "=== STEP 1.5 (alphabetical sort) ===\n",
            "first sorted ref preview:\n",
            " [5] K. Ampong, M. S. Thilakaranthna, L. Y. Gorim, Understanding the Role of Humic Acids on Crop Performance and Soil Health. Frontiers in Agronomy, 4. (2022) https://doi.org/10.3389/fagro.2022.848621 \n",
            "\n",
            "=== STEP 2 (formatted) ===\n",
            "formatted: 24\n",
            "first formatted preview:\n",
            " Ampong, K., Thilakaranthna, M.S., Gorim, L.Y. 2022. Understanding the role of humic acids on crop performance and soil health. Front. Agron. 4, 848621. https://doi.org/10.3389/fagro.2022.848621 \n",
            "\n",
            "Saved: references_llm_fixed.docx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2bcbd3e1-2f5c-419c-9b6d-6999d9050837\", \"references_llm_fixed.docx\", 39530)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}